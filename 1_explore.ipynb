{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Science\n",
    "\n",
    "Data exploration and understanding the task at hand is a fundamental step in the Machine Learning workflow.\n",
    "In this notebook, we'll take an opportunity to explore the use case, data and models we'll be using.\n",
    "\n",
    "We have been tasked with developing an application which can identify objects in static and live images. In this notebook we use a pre-trained machine learning model, and explore how it works on static photos. \n",
    "\n",
    "To begin, we import a range of python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from onnxruntime import InferenceSession\n",
    "from PIL import Image, ImageColor, ImageDraw, ImageFont\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from classes import classes\n",
    "\n",
    "\n",
    "print('Imported libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our image\n",
    "\n",
    "In the next cell we import the image we want to test our model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = 'sample-images/street.jpg'\n",
    "sample = Image.open(sample_image)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows a cat. We need to import the image as an array so the ONNX model we will use can process the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image_path, scaled_image_size=640):\n",
    "    image = cv2.imread(image_path)\n",
    "    image, ratio, dwdh = _letterbox_image(image, scaled_image_size, auto=False)\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    image = np.expand_dims(image, 0)\n",
    "    image = np.ascontiguousarray(image)\n",
    "    im = image.astype(np.float32)\n",
    "    im /= 255\n",
    "    return im, ratio, dwdh\n",
    "\n",
    "\n",
    "def _letterbox_image(\n",
    "        im, image_size, color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
    "\n",
    "    shape = im.shape[:2]\n",
    "    new_shape = image_size\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n",
    "\n",
    "    if auto:\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)\n",
    "\n",
    "    dw /= 2\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(\n",
    "        im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
    "    )\n",
    "\n",
    "    return im, r, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "converted_image, scale_factor, padding = transform(sample_image)\n",
    "converted_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in a model\n",
    "\n",
    "The model we are going to use today is the Tiny YOLO v3 model, which you can download from the ONNX Model Zoo [here](https://github.com/onnx/models/tree/main/vision/object_detection_segmentation/tiny-yolov3). The model has been trained on the [COCO](https://cocodataset.org/#home) data set, and can recognise 80 types of objects. We begin by loading in the model.\n",
    "\n",
    "The model is stored in an s3 bucket, and we connect to it using the `boto3` library. The `boto3` library was built into the object detection notebook image, which we selected from the spawner page. As such, it is already installed in our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_endpoint_url = environ.get('AWS_S3_ENDPOINT')\n",
    "s3_access_key = environ.get('AWS_ACCESS_KEY_ID')\n",
    "s3_secret_key = environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "print('Imported s3 library')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use a dedicated S3 bucket that was provisioned for you with your user ID, so let's set the S3 bucket name accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = 'object-detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    's3', endpoint_url=s3_endpoint_url,\n",
    "    aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key,\n",
    ")\n",
    "s3.download_file(s3_bucket_name, 'yolov5m.onnx', 'model.onnx')\n",
    "\n",
    "print('Downloaded model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see that this file has been added to your file directory on the left hand side of the screen.\n",
    "\n",
    "Let's now use the model to run object detection on our sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = InferenceSession('model.onnx')\n",
    "raw_result = session.run(\n",
    "    [], {'images': converted_image}\n",
    ")[0]\n",
    "raw_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(\n",
    "        prediction,\n",
    "        class_labels,\n",
    "        conf_thres=0.2,\n",
    "        iou_thres=0.6,\n",
    "        max_det=300,\n",
    "        nm=0,\n",
    "):\n",
    "    prediction = torch.Tensor(prediction)\n",
    "    bs = prediction.shape[0]\n",
    "    nc = prediction.shape[2] - nm - 5\n",
    "    xc = prediction[..., 4] > conf_thres\n",
    "\n",
    "    max_wh = 7680\n",
    "    max_nms = 30000\n",
    "\n",
    "    mi = 5 + nc\n",
    "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for xi, x in enumerate(prediction):\n",
    "        x = x[xc[xi]]\n",
    "\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        x[:, 5:] *= x[:, 4:5]\n",
    "        box = _xywh2xyxy(x[:, :4])\n",
    "        mask = x[:, mi:]\n",
    "        conf, j = x[:, 5:mi].max(1, keepdim=True)\n",
    "        x = torch.cat((box, conf, j.float(), mask), 1)[\n",
    "            conf.view(-1) > conf_thres\n",
    "        ]\n",
    "\n",
    "        n = x.shape[0]\n",
    "        if not n:\n",
    "            continue\n",
    "        elif n > max_nms:\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
    "        else:\n",
    "            x = x[x[:, 4].argsort(descending=True)]\n",
    "\n",
    "        c = x[:, 5:6] * max_wh\n",
    "        boxes = x[:, :4] + c\n",
    "        scores = x[:, 4]\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)\n",
    "        if i.shape[0] > max_det:\n",
    "            i = i[:max_det]\n",
    "\n",
    "        output[xi] = x[i]\n",
    "\n",
    "        final_boxes = np.array(output[xi][..., :4])\n",
    "        final_boxes = final_boxes.round().astype(np.int32).tolist()\n",
    "        cls_id = np.array(output[xi][..., 5], dtype=int)\n",
    "        scores = np.array(output[xi][..., 4])\n",
    "        names = [class_labels[id_] for id_ in cls_id]\n",
    "\n",
    "        results.append([final_boxes, scores, names])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _xywh2xyxy(x):\n",
    "    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def _box_iou(box1, box2, eps=1e-7):\n",
    "    (a1, a2), (b1, b2) = (\n",
    "        box1.unsqueeze(1).chunk(2, 2),\n",
    "        box2.unsqueeze(0).chunk(2, 2)\n",
    "    )\n",
    "    inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "    return inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = classes['coco']\n",
    "result = postprocess(raw_result, class_labels)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has returned arrays, each of which holds information about the detected objects. The information includes identifiers for the types of objects, coordinates locating the objects within the image, and detection scores, corresponding to how certain the model is about its prediction.\n",
    "\n",
    "We can use a few functions to help us to superimpose the information in this dictionary onto the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes, scores, classes):\n",
    "    \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
    "    colors = list(ImageColor.colormap.values())\n",
    "    class_colors = {}\n",
    "    font = ImageFont.load_default()\n",
    "    image_pil = Image.open(image)\n",
    "\n",
    "    for index, class_ in enumerate(classes):\n",
    "        box = boxes[index]\n",
    "        display_str = f'{class_}: {int(100 * scores[index])}%'\n",
    "        if class_ not in class_colors:\n",
    "            class_colors[class_] = colors[hash(class_) * 8 % len(colors)]\n",
    "        color = class_colors.get(class_)\n",
    "        _draw_bounding_box_on_image(\n",
    "            image_pil, box[0], box[1], box[2], box[3], color, font,\n",
    "            display_str_list=[display_str]\n",
    "        )\n",
    "    return image_pil\n",
    "    image_pil.show()\n",
    "\n",
    "\n",
    "def _draw_bounding_box_on_image(\n",
    "        image, xmin, ymin, xmax, ymax, color, font,\n",
    "        thickness=4, display_str_list=()):\n",
    "    \"\"\"Adds a bounding box to an image.\"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    im_width, im_height = image.size\n",
    "    width_scaling_factor = im_width / 640\n",
    "    height_scaling_factor = im_height / 640\n",
    "    (left, right, top, bottom) = (\n",
    "        xmin * width_scaling_factor,\n",
    "        xmax * width_scaling_factor,\n",
    "        ymin * height_scaling_factor,\n",
    "        ymax * height_scaling_factor,\n",
    "    )\n",
    "    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
    "               (left, top)], width=thickness, fill=color)\n",
    "\n",
    "    display_str_heights = [font.getbbox(ds)[3] for ds in display_str_list]\n",
    "    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "    if top > total_display_str_height:\n",
    "        text_bottom = top\n",
    "    else:\n",
    "        text_bottom = top + total_display_str_height\n",
    "\n",
    "    for display_str in display_str_list[::-1]:\n",
    "        _, _, text_width, text_height = font.getbbox(display_str)\n",
    "        margin = np.ceil(0.05 * text_height)\n",
    "        draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
    "                        (left + text_width, text_bottom)], fill=color)\n",
    "        draw.text((left + margin, text_bottom - text_height - margin),\n",
    "                  display_str, fill=\"black\", font=font)\n",
    "        text_bottom -= text_height - 2 * margin\n",
    "\n",
    "\n",
    "print('Defined image display functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes(sample_image, *result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! So you've seen how we can use a pre-trained model to identify objects in images. In the next notebooks, we will deploy this model using RHODS Model Serving, which allows us to use it as part of a larger application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
